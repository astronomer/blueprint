---
x-airflow-common: &airflow-common
  image: blueprint-airflow
  build:
    context: ..
    dockerfile: examples/Dockerfile
  networks:
    - airflow
  environment: &common-env-vars
    AIRFLOW__API__BASE_URL: "http://localhost:8080"
    AIRFLOW__API__PORT: 8080
    AIRFLOW__API_AUTH__JWT_SECRET: "change-me-jwt-secret-key"
    AIRFLOW__CORE__AUTH_MANAGER: airflow.api_fastapi.auth.managers.simple.simple_auth_manager.SimpleAuthManager
    AIRFLOW__CORE__SIMPLE_AUTH_MANAGER_ALL_ADMINS: "True"
    AIRFLOW__CORE__EXECUTION_API_SERVER_URL: "http://api-server:8080/execution/"
    AIRFLOW__CORE__EXECUTOR: LocalExecutor
    AIRFLOW__CORE__FERNET_KEY: ""
    AIRFLOW__CORE__LOAD_EXAMPLES: "False"
    AIRFLOW__CORE__SQL_ALCHEMY_CONN: postgresql://airflow:pg_password@postgres:5432/airflow
    AIRFLOW__DATABASE__SQL_ALCHEMY_CONN: postgresql://airflow:pg_password@postgres:5432/airflow
    AIRFLOW__SCHEDULER__STANDALONE_DAG_PROCESSOR: True
    AIRFLOW__WEBSERVER__SECRET_KEY: "change-me-webserver-secret-key"
    AIRFLOW__WEBSERVER__RBAC: "True"
    AIRFLOW__WEBSERVER__EXPOSE_CONFIG: "True"
    ASTRONOMER_ENVIRONMENT: local
    OPENLINEAGE_DISABLED: "True"
    AIRFLOW__SCHEDULER__ENABLE_HEALTH_CHECK: "true"
    # Blueprint Jinja2 templating: Environment variables
    # Accessed in .dag.yaml files as {{ env.ENV }}
    ENV: "local"
    # Blueprint Jinja2 templating: Airflow Variables
    # Accessed in .dag.yaml files as {{ var.value.<name> }}
    AIRFLOW_VAR_SOURCE_SCHEMA: "raw"
    AIRFLOW_VAR_TARGET_SCHEMA: "analytics"
    AIRFLOW_VAR_ETL_SCHEDULE: "@daily"
    # Blueprint Jinja2 templating: Airflow Connections
    # Accessed in .dag.yaml files as {{ conn.<conn_id>.host }}, {{ conn.<conn_id>.port }}, etc.
    # Format: <conn_type>://<login>:<password>@<host>:<port>/<schema>?<extra_params>
    AIRFLOW_CONN_WAREHOUSE_DB: "postgresql://etl_user:etl_password@warehouse.example.com:5432/analytics"
  volumes:
    - ./dags:/usr/local/airflow/dags
    - ./plugins:/usr/local/airflow/plugins
    - ./include:/usr/local/airflow/include
    - ./tests:/usr/local/airflow/tests
    - airflow_logs:/usr/local/airflow/logs

networks:
  airflow:
    driver: bridge

volumes:
  postgres_data:
    driver: local
  airflow_logs:
    driver: local

services:
  postgres:
    image: postgres:13
    restart: unless-stopped
    networks:
      - airflow
    ports:
      - "5432:5432"
    volumes:
      - postgres_data:/var/lib/postgresql/data
    environment:
      POSTGRES_USER: airflow
      POSTGRES_PASSWORD: pg_password
      POSTGRES_DB: airflow
    healthcheck:
      test: ["CMD", "pg_isready", "-U", "airflow"]
      interval: 10s
      retries: 5
      start_period: 5s

  db-migration:
    <<: *airflow-common
    depends_on:
      - postgres
    command:
      - airflow
      - db
      - migrate

  scheduler:
    <<: *airflow-common
    depends_on:
      - db-migration
    command:
      - airflow
      - scheduler
    restart: unless-stopped
    healthcheck:
      test: ["CMD", "curl", "--fail", "http://localhost:8974/health"]
      interval: 30s
      timeout: 10s
      retries: 5
      start_period: 30s

  dag-processor:
    <<: *airflow-common
    depends_on:
      - db-migration
    command:
      - airflow
      - dag-processor
    restart: unless-stopped
    healthcheck:
      test:
        [
          "CMD-SHELL",
          'airflow jobs check --job-type DagProcessorJob --hostname "$${HOSTNAME}"',
        ]
      interval: 30s
      timeout: 10s
      retries: 5
      start_period: 30s

  api-server:
    <<: *airflow-common
    depends_on:
      - db-migration
    command:
      - airflow
      - api-server
    restart: unless-stopped
    ports:
      - "8080:8080"
    healthcheck:
      test: ["CMD", "curl", "--fail", "http://localhost:8080/api/v2/version"]
      interval: 30s
      timeout: 10s
      retries: 5
      start_period: 30s

  triggerer:
    <<: *airflow-common
    depends_on:
      - db-migration
    command:
      - airflow
      - triggerer
    restart: unless-stopped
    healthcheck:
      test:
        [
          "CMD-SHELL",
          'airflow jobs check --job-type TriggererJob --hostname "$${HOSTNAME}"',
        ]
      interval: 30s
      timeout: 10s
      retries: 5
      start_period: 30s
